\documentclass{article}
\usepackage{amsmath}

\begin{document}
\title{PAC Learning}
\author{Kinh Tieu}
\date{June 4, 2017}
\maketitle

\section{Introduction}

The PAC (Probabably Approximately Correct) machine learning framework tackles the question of what is efficiently learnable.

Let $\mathcal{X}$ be the input space and $\mathcal{Y} = \{0, 1\}$ be the output space.  A concept $c$ is a subset of $\mathcal{X}$ or a mapping $c: \mathcal{X} \mapsto \mathcal{Y}$.  A concept class $C$ is a set of concepts.  The learner is given a sample $S$ consisting of examples $(x_i, y_i)$ draw from a distribution $D$ and must select an hypothesis $h$ from a set of hypotheses $H$ to minimize the risk $R(h)$.  The risk is the generalization error:
$$ R(h) = P_D[h(x) \ne c(x)] = E_D[1_{h(x) \ne c(x)}]. $$

A concept class $C$ is PAC-learnable if there exists an algorithm $A$ such that for any $\epsilon > 0$, $\delta > 0$, $c \in C$, and $D$, the following holds for any sample size $m \ge poly(1/\epsilon, 1/\delta)$:
$$ P_{D^m}[R(h) \le \epsilon] \ge 1 - \delta. $$
The idea is that $A$ can probably approximate $c$ with a polynomial sample size.

\section{Simple Example of Positive Rays}

Let $C$ be the set of all positive rays on the real line, so $\mathcal{X} = R$ and $c_\theta(x) = 1_{x \ge \theta}$.  The algorithm $A$ will use $H = C$, and computes the following two values:
\begin{align*}
x_0 = \arg \max_{x_i} y_i &= 0 \\
x_1 = \arg \min_{x_i} y_i &= 1.
\end{align*}
The algorithm $A$ outputs $h_\theta$ where $x_0 < \theta < x_1$.

% Relate this to Stoller splits.
\end{document}
