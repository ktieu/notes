\documentclass{article}
\usepackage{amsmath, graphicx}

\begin{document}
\title{Neural Networks}
\author{}
\date{}
\maketitle

\section{A Single Neuron}

The best place to start is with the simplest neural network, a single neuron, which also is equivalent to a linear classifier.  With a logistic activation function, a single neuron is logistic regression, and our presentation with focus on this particular case.

The scalar output $y$ is a logistic function of the inner product of the input $x$ and a vector of weights $w$:

\begin{equation}
y = \frac{1}{1 + e^{-w^T x}}.
\end{equation}

We can also write this in terms of the activation $a$:

\begin{equation}
y = \frac{1}{1 + e^{-a}},
\end{equation}

where,

\begin{align}
a &= \sum_{i = 0}^I w_i x_i \\
&= \sum_{i = 1}^I w_i x_i + b,
\end{align}

where the bias $b = w_0$.

\subsection{Learning}

Given labeled data $\{(x^1, t^1), (x^2, t^2), \ldots, (x^N, t^N)\}$, a single neuron can be trained by minimizing the sample Kullback-Leibler divergence:
\begin{equation}
E = - \sum_n t^n \ln y^n + (1 - t^n) \ln (1 - y^n) 
\end{equation}

\begin{align}
\frac{\partial E}{\partial w} &= - \sum_n t^n \frac{y_i (1 - y_i) x_i}{y_i} + (1 - t_i) \frac{(-y_i (1 - y_i) x_i)}{ (1 - y_i)} \\
&= - \sum_i t_i (1 - y_i) x_i + (1 - t_i) (-y_i x_i) \\
&= - \sum_i (t_i - y_i) x_i
\end{align}

Note that the gradient is the error on the example $e_i = (t_i - y_i)$ times the example $x_i$.

\subsection{Derivation of Gradient}

\begin{align}
\frac{\partial y}{\partial w} & = \frac{\partial}{\partial w}\frac{1}{1 + e^{-x}} \\
&= \frac{\partial}{\partial w}\frac{e^x}{1 + e^x} \\
&= \frac{e^x (1 + e^x) - e^{2x}}{(1 + e^x)^2} \\
&= \frac{e^x}{(1 + e^x)^2} \\
&= \frac{e^x}{1 + e^x} \frac{1}{1 + e^x} \\
&= \frac{1}{1 + e^{-x}} \frac{e^{-x}}{1 + e^{-x}} \\
&= \frac{1}{1 + e^{-x}} \left(1 - \frac{1}{1 + e^{-x}} \right) \\
\end{align}

\subsection{Statistical Interpretation}

A single neuron turns out to be the optimal classifier for some simple statistical problems such as denoising a codeword generated by one of two sources.  This problem can be analyzed by considering the ratio of posterior probabilities for the source $y$ given the noisy example $x$:

\begin{equation}
\frac{p(y_1 | x)}{p(y_0 | x)} = \frac{p(x | y_1)}{p(x | y_0)} \frac{p(y_1)}{p(y_0)}.
\end{equation}

By specializing to a Gaussian noise distribution:
\begin{equation}
p(x | y) = \left| \frac{\Omega}{2 \pi} \right|^{1/2} \exp \left( -\frac{1}{2} (x - \mu_y)^T \Omega (x - \mu_y) \right),
\end{equation}

where $\mu$ is the source codeword and $\Omega$ is the precision matrix.  Working out the algebra,

\begin{align}
\frac{p(y_1 | x)}{p(y_0 | x)} &= \exp\left(-\frac{1}{2}(x-\mu_1)^T\Omega(x-\mu_1) + \frac{1}{2}(x-\mu_0)^T\Omega(x-\mu_0) + \ln \frac{p(y_1)}{p(y_0)} \right) \\
& =  \exp \left( x^T \Omega (\mu_1 - \mu_0) - \frac{1}{2} (\mu_1^T \Omega \mu_1 - \mu_0^T \Omega \mu_0) - \ln \frac{p(y_1)}{p(y_0)} \right) \\
& =  \exp (w^T x + b),
\end{align}

where

\begin{align}
w &= \Omega (\mu_1 - \mu_0) \\
b &= - \frac{1}{2} (\mu_1^T \Omega \mu_1 - \mu_0^T \Omega \mu_0) - \ln \frac{p(y_1)}{p(y_0)}\
\end{align}

Note that the weight vector $w$ is the difference between the sources transformed by the precision matrix.\marginpar{show a figure of this}
\marginpar{also two Gaussians with same mean but different variances}
\marginpar{also linear decoder on binary symmetric channel}

\end{document}
